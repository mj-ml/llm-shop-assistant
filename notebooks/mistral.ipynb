{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Mistral test\n",
   "id": "c54672b212659962"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from mistralai import Mistral\n",
    "\n",
    "from mistral_key import key\n",
    "\n",
    "api_key = key\n",
    "model = \"mistral-small-2409\"\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "context = \"Shipping costs 5€ in the EU, 10€ outside of the EU in Europe, 15€ to the US and Canada and 20€ everywhere else.\"\n",
    "\n",
    "question = 'How much is shipping to Somalia?'\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Context information is below.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "You are a shop assistant. Be polite with the customer. Don't give any explanation. Reply briefly.\n",
    "Given the context information and not prior knowledge, answer the query. \n",
    "Query: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "chat_response = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "ans = chat_response.choices[0].message.content\n",
    "print(ans)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt_template = \"\"\"\n",
    "    You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "    Your task is to analyze the relevance of the generated answer to the given question.\n",
    "    Based on the relevance of the generated answer, you will classify it\n",
    "    as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "    Here is the data for evaluation:\n",
    "\n",
    "    Question: {question}\n",
    "    Generated Answer: {answer}\n",
    "\n",
    "    Please analyze the content and context of the generated answer in relation to the question\n",
    "    and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "    {{\n",
    "      \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "      \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "    }}\n",
    "    \"\"\".strip()\n",
    "\n",
    "prompt_eval = prompt_template.format(question=question, answer=ans)\n",
    "chat_response_eval = client.chat.complete(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt_eval,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "eval_ans = chat_response_eval.choices[0].message.content\n",
    "print(eval_ans)"
   ],
   "id": "f76a6fc6150c467f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ground truth generation\n",
   "id": "56db1d9445130e95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "questions_path = \"data/questions.json\"\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(questions_path, 'r') as file:\n",
    "    questions = json.load(file)\n",
    "\n",
    "questions"
   ],
   "id": "f86a87809aa916a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import hashlib\n",
    "\n",
    "docs = questions['questions']\n",
    "\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['question']}-{doc['answer'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n",
    "\n",
    "\n",
    "for doc in docs:\n",
    "    doc['id'] = generate_document_id(doc)\n",
    "\n",
    "pd.DataFrame(docs).to_csv('data/questions-raw.csv', index=False)\n"
   ],
   "id": "6e951a26d39c9141",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompt_template = \"\"\"\n",
    "Emulate a client who is planning to buy at our shop.\n",
    "Formulate 5 questions this client might ask based on a FAQ record. The record\n",
    "should contain the answer to the questions, and the questions should be complete and not too short.\n",
    "If possible, use as fewer words as possible from the record. \n",
    "\n",
    "The record:\n",
    "\n",
    "question: {question}\n",
    "answer: {answer}\n",
    "\n",
    "Provide the output in parsable list:\n",
    "\n",
    "[\"question1\", \"question2\", ...]\n",
    "\"\"\".strip()"
   ],
   "id": "14cd08127e125cd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_question(doc):\n",
    "    prompt = prompt_template.format(**doc)\n",
    "\n",
    "    chat_response = client.chat.complete(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    ans = chat_response.choices[0].message.content\n",
    "    return ans"
   ],
   "id": "f1501e3f0ab88511",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7f8754410d972f0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "gen_questions = {}\n",
    "\n",
    "for doc in tqdm(docs[:1]):\n",
    "    q = generate_question(doc)\n",
    "    gen_questions[doc['id']] = json.loads(q)\n",
    "    time.sleep(6)"
   ],
   "id": "22ef814afaabba79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final = []\n",
    "for doc_id, questions in gen_questions.items():\n",
    "    for question in questions:\n",
    "        final.append((doc_id, question))\n"
   ],
   "id": "5bf89790f3af1c1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_questions = pd.DataFrame(final, columns=['doc_id', 'question'])\n",
    "# df_questions.to_csv(\"data/ground_truth.csv\", index=False)\n"
   ],
   "id": "e526090803bdf28b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_questions",
   "id": "cb4b2037c2e4e438",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "625ce3e9a08a7443",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
